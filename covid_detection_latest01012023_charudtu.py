# -*- coding: utf-8 -*-
"""Covid_detection_latest01012023_charudtu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sg6nA1U-KVdHGibxlBHnEDyi1LMJO171
"""

from google.colab import drive
drive.mount("/content/gdrive")

import os

import seaborn as sns

import time

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

os.listdir()

import pandas as pd
df = pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/DATA_FINAL.csv')

df

df.shape

#Display the summary statistics for independent variables
df.iloc[:,1:].describe()

df

df.rename(columns={'Alarm Raised': 'Alarm_Raised'}, inplace=True)

print(df.Alarm_Raised.value_counts())

sns.countplot(x='Alarm_Raised',data=df)
plt.show()

X = df.iloc[:, ~df.columns.isin(['Alarm Raised', 'PID'])].values
Y = df.iloc[:, df.columns=='Alarm Raised'].values

df

df.info()

Y

Y.shape

X

X.shape

from sklearn.preprocessing import LabelEncoder, OneHotEncoder

labelencoder_X = LabelEncoder()
X[:, 0] = labelencoder_X.fit_transform(X[:, 0])

labelencoder_Y = LabelEncoder()
Y = labelencoder_Y.fit_transform(Y)

Y.ravel()

Y.shape

Y

Y.ndim

Y.size

"""**SPLITTING DATA SET**"""

from sklearn.model_selection import KFold, cross_val_score, train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=0)

subset=['BT','OSR','HR','RR','BP','Alarm Raised']

df1=df[subset]

df1.head()

sns.set_style("whitegrid");
sns.pairplot(df1, hue="Alarm Raised", size=3);
plt.show()

sns.boxplot(x='Alarm_Raised',y='RR',data=df1)

"""SUBPLOT"""

import matplotlib.pyplot as plt
x=[1,2,3,4]
y=[1,2,3,4]
plt.subplot(2,2,1)
plt.plot(x,y,color='r')
plt.subplot(2,2,2)
plt.pie([1],colors='r')
plt.subplot(2,2,3)
plt.pie(x)
plt.subplot(2,2,4)
x2=['a','q','w','c']
y2=[30,40,50,68]
plt.bar(x2,y2)
plt.show()
#plt.show()

x=[1,2,3,4,5]
a=[2,3,1,4,5]
a1=[15,20,25,10,15]
plt.stackplot(x,a,a1)
plt.show()

plt.subplot(2,2,3)
plt.pie(x)
#plt.show()

plt.subplot(2,2,4)
x2=['a','q','w','c']
y2=[30,40,50,68]
plt.bar(x2,y2)
plt.show()

sns.boxplot(x='Alarm_Raised',y='BP',data=df1)

sns.boxplot(x='Alarm_Raised',y='HR',data=df1)

sns.boxplot(x='Alarm_Raised',y='OSR',data=df1)

sns.boxplot(x='Alarm_Raised',y='BT',data=df1)

sns.violinplot(x='Alarm_Raised',y='BP',data=df1,size=6)

sns.FacetGrid(df1,hue='Alarm_Raised',size=5).map(sns.distplot,'OSR').add_legend()

sns.jointplot(data=df1, x="BP", y="RR",hue='Alarm_Raised')

sns.jointplot(data=df1, x="BP", y="OSR", hue="Alarm_Raised", kind="kde")

sns.jointplot(data=df1, x="BP", y="OSR", kind="reg")

# for x in range(0,20):
# 	print("[")
# 	for y in range(0,19):
# 		print(X_train[x][y],",")
# 	print("]")

"""**SCALING DATASET**"""

from sklearn.preprocessing import StandardScaler
sc_X = StandardScaler()
X_train = sc_X.fit_transform(X_train)
X_test = sc_X.transform(X_test)

sc_X.mean_

sc_X.var_

"""# New Section

**LOGISTIC REGRESSION**
"""

from sklearn.linear_model import LogisticRegression
classifier_LG = LogisticRegression(random_state = 0)
#classifier_LG.fit(X_train, Y_train)

t=0
for i in range(100):
  start = time.time()*1000
  classifier_LG.fit(X_train, Y_train)
  stop = time.time()*1000
  t=t+stop - start
av=t/100
print(f"Training time: {av}ms")

t1=0
for i in range(100):
  start = time.time()*1000
  classifier_LG.predict(X_test)
  stop = time.time()*1000
  t1=t1+stop-start
av1=t1/100
print(f"Infeence time: {av1}ms")

y_pred_LG = classifier_LG.predict(X_test)

#Making of confusion matrix
from sklearn.metrics import confusion_matrix
cm_LG = confusion_matrix(Y_test, y_pred_LG)

"""PLOTTING Confusion Mtarix"""

y_p_LG = classifier_LG.decision_function(X_test)

cm_LG

"""

```
# This is formatted as code
```

**Plotting Confusion Matrix**"""

import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
classifier_LG.classes_
cm=confusion_matrix(Y_test, y_pred_LG,labels=classifier_LG.classes_)
print(cm)

#fig, ax = plt.subplots(figsize=(8, 6))
cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['No Alarm Raised','Alarm Raised'])
cm_disp.plot()
#plt.savefig("cm_plot", dpi=300)
#plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
# create confusion matrix from predictions
fig, ax = plt.subplots(figsize=(8, 6))
ConfusionMatrixDisplay.from_predictions(
    Y_test, y_pred_LG, labels=classifier_LG.classes_, ax=ax, colorbar=True
)
plt.savefig("cm_plot", dpi=300)
plt.show()

from sklearn.metrics import accuracy_score
accuracy_score(Y_test, y_pred_LG)

from sklearn.metrics import precision_recall_fscore_support
precision_recall_fscore_support(Y_test, y_pred_LG, average='macro')

pip install scikit-plot

import scikitplot as skplt

import sklearn
#from sklearn.datasets import load_digits, load_boston, load_breast_cancer
from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA

import matplotlib.pyplot as plt

import sys
import warnings
warnings.filterwarnings("ignore")

print("Scikit Plot Version : ", skplt.__version__)
print("Scikit Learn Version : ", sklearn.__version__)
print("Python Version : ", sys.version)

sklearn.metrics.SCORERS.keys()

"""**PLOTTING LEARNING GRAPH of LR**"""

skplt.estimators.plot_learning_curve(LogisticRegression(), X, Y,
                                     cv=5, shuffle=False, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="Classification Learning Curve");

fig = plt.figure(figsize=(15,6))

ax1 = fig.add_subplot(121)
skplt.metrics.plot_confusion_matrix(Y_test, y_pred_LG,
                                    title="Confusion Matrix",
                                    cmap="Oranges",
                                    ax=ax1)

ax2 = fig.add_subplot(122)
skplt.metrics.plot_confusion_matrix(Y_test, y_pred_LG,
                                    normalize=True,
                                    title="Confusion Matrix",
                                    cmap="Purples",
                                    ax=ax2);

Y_test_probs = classifier_LG.predict_proba(X_test)

skplt.metrics.plot_roc_curve(Y_test, Y_test_probs,
                       title="ROC Curve", figsize=(12,6));

skplt.metrics.plot_precision_recall_curve(Y_test, Y_test_probs,
                       title="Precision-Recall Curve", figsize=(12,6));

pca = PCA(random_state=1)
pca.fit(X)

skplt.decomposition.plot_pca_component_variance(pca, figsize=(8,6));

skplt.decomposition.plot_pca_2d_projection(pca, X, Y,
                                           figsize=(10,10),
                                           cmap="tab10");

"""**KNN Classifier**"""

from sklearn.neighbors import KNeighborsClassifier
classifier_KNN = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p = 2)
#classifier_KNN.fit(X_train, Y_train)

t=0
for i in range(100):
  start = time.time()*1000
  classifier_KNN.fit(X_train, Y_train)
  stop = time.time()*1000
  t=t+stop - start
av=t/100
print(f"Training time: {av}ms")

t=0
for i in range(100):
  start = time.time()*1000
  classifier_KNN.fit(X_train, Y_train)
  stop = time.time()*1000
  t=t+stop - start
av=t/100
print(f"Training time: {av}ms")

t=0
for i in range(5):
  start = time.time()*1000
  classifier_KNN.fit(X_train, Y_train)
  stop = time.time()*1000
  t=t+stop - start
av=t/5
print(f"Training time: {av}ms")

t=0
for i in range(5):
  start = time.time()*1000
  classifier_KNN.fit(X_train, Y_train)
  stop = time.time()*1000
  print(f"Time : {stop-start}ms")
  t=t+stop - start
av=t/5
print(f"Training time: {av}ms")

t=0
for i in range(5):
  start = time.time()*1000
  classifier_KNN.fit(X_train, Y_train)
  stop = time.time()*1000
  print(f"Time : {stop-start}ms")
  t=t+stop - start
av=t/5
print(f"Training time: {av}ms")

t1=0
for i in range(5):
  start1 = time.time()*1000
  y_pred_KNN = classifier_KNN.predict(X_test)
  stop1 = time.time()*1000
  t=t+stop1 - start1
av1=t/5

print(f"Inference time: {av1}ms")

t1=0
for i in range(5):
  start1 = time.time()*1000
  y_pred_KNN = classifier_KNN.predict(X_test)
  stop1 = time.time()*1000
  t=t+stop1 - start1
av1=t/5

print(f"Inference time: {av1}ms")

cm_KNN = confusion_matrix(Y_test, y_pred_KNN)
print(cm_KNN)

cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm_KNN, display_labels=['No Alarm Raised','Alarm Raised'])
cm_disp.plot()

cm_KNN

accuracy_score(Y_test, y_pred_KNN)

precision_recall_fscore_support(Y_test, y_pred_KNN, average='macro')

skplt.estimators.plot_learning_curve(classifier_KNN, X, Y,
                                     cv=5, shuffle=False, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="Classification Learning Curve");

"""**SUPPORT VECTOR MACHINE**"""

from sklearn.svm import SVC
classifier_SVM = SVC(kernel = 'linear', random_state = 0)
#classifier_SVM.fit(X_train, Y_train)

t=0
for i in range(100):
  start = time.time()*1000
  classifier_SVM.fit(X_train, Y_train)
  stop = time.time()*1000
  t=t+stop - start
av=t/5
print(f"Training time: {av}ms")

t1=0
for i in range(5):
  start1 = time.time()*1000
  y_pred_SVM = classifier_SVM.predict(X_test)
  stop1 = time.time()*1000
  t=t+stop1 - start1
av1=t/5

print(f"Inference time: {av1}ms")

start = time.time()*1000
classifier_SVM.fit(X_train, Y_train)
stop = time.time()*1000
print(f"Training time: {stop - start}s")

y_pred_SVM = classifier_SVM.predict(X_test)

cm_SVM = confusion_matrix(Y_test, y_pred_SVM)

cm_SVM

cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm_SVM, display_labels=['No Alarm Raised','Alarm Raised'])
cm_disp.plot()

accuracy_score(Y_test, y_pred_SVM)

precision_recall_fscore_support(Y_test, y_pred_SVM, average='macro')

skplt.estimators.plot_learning_curve(classifier_SVM, X, Y,
                                     cv=7, shuffle=True, scoring="recall",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="Classification Learning Curve");

"""**DECISION TREE**"""

#decision Tree Classifier
from sklearn.tree import DecisionTreeClassifier
classifier_DT = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)
classifier_DT.fit(X_train, Y_train)

t=0
for i in range(5):
  start = time.time()*1000
  classifier_DT.fit(X_train, Y_train)
  stop = time.time()*1000
  t=t+stop - start
av=t/5
print(f"Training time: {av}ms")

t=0
for i in range(5):
  start = time.time()*1000
  classifier_DT.predict(X_test)
  stop = time.time()*1000
  t=t+stop - start
av=t/5
print(f"INFERENCE time: {av}ms")

y_pred_DT = classifier_DT.predict(X_test)

cm_DT = confusion_matrix(Y_test, y_pred_DT)

cm_DT

cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm_DT, display_labels=['No Alarm Raised','Alarm Raised'])
cm_disp.plot()

accuracy_score(Y_test, y_pred_DT)

precision_recall_fscore_support(Y_test, y_pred_DT, average='macro')

skplt.estimators.plot_learning_curve(classifier_DT, X, Y,
                                     cv=7, shuffle=True, scoring="accuracy",
                                     n_jobs=-1, figsize=(6,4), title_fontsize="large", text_fontsize="large",
                                     title="Classification Learning Curve");

"""**RANDOM FOREST **"""

#Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
classifier_RF = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)
#classifier_RF.fit(X_train, Y_train)

t=0
for i in range(5):
  start = time.time()*1000
  classifier_RF.fit(X_train, Y_train)
  stop = time.time()*1000
  t=t+stop - start
av=t/5
print(f"Training time: {av}ms")

start = time.time()*1000
classifier_RF.fit(X_train, Y_train)
stop = time.time()*1000
print(f"Training time: {stop - start}s")

t=0
for i in range(5):
  start = time.time()*1000
  classifier_RF.predict(X_test)
  stop = time.time()*1000
  t=t+stop - start
av=t/5
print(f"INFERENCE time: {av}ms")

y_pred_RF = classifier_RF.predict(X_test)

cm_RF = confusion_matrix(Y_test, y_pred_RF)

cm_RF

cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm_RF, display_labels=['No Alarm Raised','Alarm Raised'])
cm_disp.plot()

accuracy_score(Y_test, y_pred_RF)

precision_recall_fscore_support(Y_test, y_pred_RF, average='macro')

"""**Gradient Boosting**"""

from sklearn.ensemble import GradientBoostingRegressor

classifier_gbr = GradientBoostingRegressor(n_estimators = 200, max_depth = 1, random_state = 0)

Y_train.shape

t=0
for i in range(5):
  start = time.time()*1000
  classifier_gbr.fit(X_train, Y_train)
  stop = time.time()*1000
  t=t+stop - start
av=t/5
print(f"Training time: {av}ms")

t=0
for i in range(5):
  start = time.time()*1000
  classifier_gbr.predict(X_test)
  stop = time.time()*1000
  t=t+stop - start
av=t/5
print(f"INFERENCE time: {av}ms")

start = time.time()*1000
classifier_gbr.fit(X_train, Y_train)
stop = time.time()*1000
print(f"Training time: {stop - start}s")

y_pred_GBR = classifier_gbr.predict(X_test)

cm_GBR = confusion_matrix(Y_test, y_pred_GBR)
print(cm_GBR)

cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm_GBR, display_labels=['No Alarm Raised','Alarm Raised'])
cm_disp.plot()

"""Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
classifier_NB = GaussianNB()
#classifier_NB.fit(X_train, y_train)

t=0
for i in range(5):
  start = time.time()*1000
  classifier_NB.fit(X_train, Y_train)
  stop = time.time()*1000
  t=t+stop - start
av=t/5
print(f"Training time: {av}ms")

t=0
for i in range(5):
  start = time.time()*1000
  classifier_NB.predict(X_test)
  stop = time.time()*1000
  t=t+stop - start
av=t/5
print(f"INFERENCE time: {av}ms")

start = time.time()*1000
classifier_NB.fit(X_train, Y_train)
stop = time.time()*1000
print(f"Training time: {stop - start}s")

y_pred_NB= classifier_NB.predict(X_test)

cm_NB = confusion_matrix(Y_test, y_pred_NB)
print(cm_NB)

cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm_NB, display_labels=['No Alarm Raised','Alarm Raised'])
cm_disp.plot()

accuracy_score(Y_test, y_pred_NB)

precision_recall_fscore_support(Y_test, y_pred_NB, average='macro')

"""**Feature Importance Plotting using random forest and Gradient Boosting**"""

fig = plt.figure(figsize=(15,6))

ax1 = fig.add_subplot(121)
skplt.estimators.plot_feature_importances(classifier_RF, feature_names=df.columns,
                                         title="Random Forest Regressor Feature Importance",
                                         x_tick_rotation=90, order="ascending",
                                         ax=ax1);

ax2 = fig.add_subplot(122)
skplt.estimators.plot_feature_importances(classifier_gbr, feature_names=df.columns,
                                         title="Gradient Boosting Classifier Feature Importance",
                                         x_tick_rotation=90,
                                         ax=ax2);

plt.tight_layout()

"""**ANN**"""

#ANN
import keras
from keras.models import Sequential
from keras.layers import Dense

classifier_ANN = Sequential()
classifier_ANN.add(Dense(units = 11,  activation = 'relu', input_dim = 19))
classifier_ANN.add(Dense(units = 11, activation = 'relu'))
classifier_ANN.add(Dense(units = 1, activation = 'sigmoid'))

classifier_ANN.compile(optimizer= 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

classifier_ANN.fit(X_train, Y_train, batch_size=10, epochs=100)

classifier_ANN.summary()

y_pred_ANN = classifier_ANN.predict(X_test)

# import numpy as np
# y_pred_ANN = np.array(y_pred_ANN)
# y_pred_ANN = y_pred_ANN.round()

y_pred_ANN

output = []
for i in y_pred_ANN:
  max_index = 0
  mx = 0
  count = 0
  for j in i:
    if mx < j:
      max_index = count
      mx = j
    count+=1
  output.append(max_index)

y_pred_ANN

Y_test

cm_ANN = confusion_matrix(Y_test, output)

accuracy_score(Y_test, output)

precision_recall_fscore_support(Y_test, output, average='macro')

cm_ANN

classifier_ANN.save('/content/predtest.h5')

from tensorflow.python import keras
import tensorflow

modelnew = keras.models.load_model('/content/predtest.h5')
converter=tensorflow.lite.TFLiteConverter.from_keras_model(modelnew)
tflite_model = converter.convert()
open("converted_model.tflite", "wb").write(tflite_model)

X_test[1]

y_pred_ANN[499]

X_test[499]

X_test[1]

from keras.models import Sequential
from keras.layers import Dense, Conv1D, MaxPooling1D, Dense, Flatten, Dropout

Y_train.shape

X_train_CNN = X_train.reshape(X_train.shape[0],X_train.shape[1],1)
X_test_CNN = X_test.reshape(X_test.shape[0],X_test.shape[1],1)
# Y_train_CNN = Y_train.reshape(Y_train.shape[0],3)

num_filters = 32
filter_size = 3
pool_size = 2


model = Sequential([
  Conv1D(16, filter_size,activation = 'relu' ,input_shape=(19, 1)),
  Conv1D(32, filter_size,activation = 'relu' ),
  MaxPooling1D(pool_size=pool_size),
  Flatten(),
#   Dense(10, activation='relu'),
#   Dropout(0.2),
  Dense(3, activation='sigmoid'),
])

model.compile(
  'rmsprop',
  loss='binary_crossentropy',
  metrics=['accuracy'],
)

model.fit(x = X_train_CNN, y = Y_train, epochs = 10)

model.summary()

Y_test_CNN= model.predict(X_test_CNN)
#

output_CNN = []
for i in Y_test_CNN:
  max_index = 0
  mx = 0
  count = 0
  for j in i:
    if mx < j:
      max_index = count
      mx = j
    count+=1
  output_CNN.append(max_index)

output_CNN

confusion_matrix_CNN = confusion_matrix(Y_test, output_CNN)
confusion_matrix_CNN

!pip install scikit-learn==1.0.2

# Visualizing 6-D mix data using scatter charts
# leveraging the concepts of hue, size, depth and shape
fig = plt.figure(figsize=(8, 6))
t = fig.suptitle('Wine Residual Sugar - Alcohol Content - Acidity - Total Sulfur Dioxide - Type - Quality', fontsize=14)
ax = fig.add_subplot(111, projection='3d')

xs = list(df1['BP'])
ys = list(df1['RR'])
zs = list(df1['OSR'])
data_points = [(x, y, z) for x, y, z in zip(xs, ys, zs)]

ss = list(df1['HR'])
colors = ['red' if wt == 'red' else 'yellow' for wt in list(df1['Alarm_Raised'])]
markers = [',' if q == 'high' else 'x' if q == 'medium' else 'o' for q in list(df1['BT'])]

for data, color, size, mark in zip(data_points, colors, ss, markers):
    x, y, z = data
    ax.scatter(x, y, z, alpha=0.4, c=color, edgecolors='none', s=size, marker=mark)

ax.set_xlabel('Residual Sugar')
ax.set_ylabel('Alcohol')
ax.set_zlabel('Fixed Acidity')